# AWS Glue Lab Exercise: Data Integration and Preparation

## Overview

In this lab, you will learn how to integrate and prepare data using AWS Glue. You’ll start by setting up an S3 bucket containing sample data, then use a Glue crawler to automatically discover and catalog the data. Finally, you will create an ETL job using AWS Glue Studio to transform and prepare your data for analysis.

## Objectives

- **Data Ingestion:** Upload sample data files (CSV/JSON) to an S3 bucket.
- **Cataloging:** Use AWS Glue Crawlers to build a Data Catalog.
- **ETL Processing:** Create and run an ETL job using AWS Glue Studio.
- **Validation:** Verify the transformation and storage of the processed data.
- **Cleanup:** Remove temporary resources to avoid additional costs.

## Prerequisites

- An AWS account with permissions to create S3 buckets, Glue resources, and IAM roles.
- Basic familiarity with AWS Glue, S3, and IAM.
- (Optional) AWS CLI installed for advanced tasks.
- Sample data file(s) prepared locally (e.g., a CSV file named `sales_data.csv`).

---

## Step 1: Setting Up the Environment

### 1.1 Create an S3 Bucket and Upload Data

1. **Log in to the AWS Management Console.**
2. **Create an S3 Bucket:**
   - Go to the S3 service.
   - Click **Create bucket** and provide a unique name (e.g., `aws-glue-lab-data`).
   - Configure settings as needed and create the bucket.
3. **Upload Sample Data:**
   - Create a CSV file named `sales_data.csv` with the following content:

     ```csv
     order_id,order_date,customer,amount
     1,2022-01-01,John Doe,100.50
     2,2022-01-02,Jane Smith,200.75
     3,2022-01-03,Jim Brown,150.00
     ```

   - Upload this file to a folder (e.g., `/raw/`) in your bucket.

### 1.2 Configure an IAM Role for AWS Glue

1. **Create an IAM Role:**
   - Navigate to the IAM service.
   - Create a new role with the **Glue** service as the trusted entity.
2. **Attach Policies:**
   - Attach policies such as **AmazonS3FullAccess** and **AWSGlueServiceRole** (or custom policies with least privilege) so Glue can access your S3 bucket and manage its resources.
3. **Name the Role:**
   - For example, name it `AWSGlueServiceRole-Lab`.

---

## Step 2: Cataloging Data with AWS Glue Crawlers

### 2.1 Create a New Crawler

1. **Navigate to the AWS Glue Console:**
   - Under **Data Catalog**, click **Crawlers**.
2. **Add a New Crawler:**
   - Click **Add crawler**.
   - **Name:** Enter `sales-data-crawler`.
   - **Data Store:** Choose **S3** and specify the S3 path where your sample data is stored (e.g., `s3://aws-glue-lab-data/raw/`).
   - **IAM Role:** Select the IAM role you created earlier (`AWSGlueServiceRole-Lab`).
   - **Output:** Create or select a database—for example, create a new database named `sales_data_db`.
   - **Schedule:** For this lab, you can run the crawler on demand.
3. **Run the Crawler:**
   - After configuration, run the crawler.
   - Once complete, verify that the crawler has created a table in the `sales_data_db` with the expected schema.

### 2.2 Verify the Data Catalog

1. **Check the Catalog:**
   - In the Glue console, click on **Databases** and then open `sales_data_db`.
   - Review the table (e.g., `sales_data`) and verify that columns such as `order_id`, `order_date`, `customer`, and `amount` have been correctly inferred.

---

## Step 3: Building an ETL Job with AWS Glue Studio

### 3.1 Create an ETL Job

1. **Open AWS Glue Studio:**
   - In the AWS Glue console, click **AWS Glue Studio**.
2. **Create a New Job:**
   - Click **Create job** and choose the **Visual with a source and target** option.
3. **Configure the Data Source:**
   - Select the table generated by your crawler (from `sales_data_db`).
4. **Add a Transform Step (Optional):**
   - For instance, add a **Mapping** transform to modify data types or rename columns.
5. **Set the Data Target:**
   - Choose S3 as your target.
   - Specify a destination path (e.g., `s3://aws-glue-lab-data/transformed_data/`).

### 3.2 Configure Job Properties

- **Job Name:** `sales_data_transformation`
- **IAM Role:** Use `AWSGlueServiceRole-Lab`.
- **Temporary Directory:** Specify an S3 path for temporary files (e.g., `s3://aws-glue-lab-data/temp/`).

### 3.3 Review and (Optional) Edit the Script

AWS Glue Studio will auto-generate a Spark script. You can edit it if you want to include custom transformations. Below is an example Python (PySpark) snippet:

```python
import sys
from awsglue.transforms import *
from awsglue.utils import getResolvedOptions
from pyspark.context import SparkContext
from awsglue.context import GlueContext
from awsglue.job import Job

args = getResolvedOptions(sys.argv, ['JOB_NAME'])
sc = SparkContext()
glueContext = GlueContext(sc)
spark = glueContext.spark_session
job = Job(glueContext)
job.init(args['JOB_NAME'], args)

# Load data from the Data Catalog
datasource0 = glueContext.create_dynamic_frame.from_catalog(
    database="sales_data_db", 
    table_name="sales_data"
)

# Apply mapping to ensure correct data types
applymapping1 = ApplyMapping.apply(frame=datasource0, mappings=[
    ("order_id", "long", "order_id", "long"), 
    ("order_date", "string", "order_date", "date"), 
    ("customer", "string", "customer", "string"), 
    ("amount", "double", "amount", "double")
])

# Write transformed data to S3 in CSV format
datasink2 = glueContext.write_dynamic_frame.from_options(
    frame=applymapping1, 
    connection_type="s3", 
    connection_options={"path": "s3://aws-glue-lab-data/transformed_data/"},
    format="csv"
)

job.commit()
```

### 3.4 Run and Monitor the Job
- **Save and Run:**
    - Save your job and click Run.
- **Monitor Execution:**
    - Use the Glue console and CloudWatch logs to track job progress and troubleshoot any issues.

-----

## Step 4: Validating the Output
- **Check the Output Data:**
    - Navigate to the S3 bucket and locate the transformed_data folder.
    - Verify that the transformed CSV files have been created.
- **Data Verification:**
    - Download a file and check that the data (and any applied transformations) are correct.

---
## Step 5: Cleanup
- **Remove Temporary Resources:**
    - Delete or archive the temporary S3 directories (temp, transformed_data) if they’re no longer needed.
- **Consider Deleting:**
    - If this lab is complete, you may also remove the crawler and ETL job to avoid incurring ongoing costs.